{"cells":[{"cell_type":"code","source":["# Mount into drive\n","\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n","\n","%cd '/content/drive/MyDrive/ColabNotebooks/gt_omscs_ml/deep_learning/Convolutionalists/FoodforDeepThought'\n","\n","!pip install -q condacolab\n","import condacolab\n","condacolab.install()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"OLE73T11EpzF","executionInfo":{"status":"ok","timestamp":1733803708985,"user_tz":360,"elapsed":11252,"user":{"displayName":"Scott Schmidl","userId":"07500865660181256076"}},"outputId":"fead90a3-2e2a-4a95-b7b2-b4e1b8757c29"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/ColabNotebooks/gt_omscs_ml/deep_learning/Convolutionalists/FoodforDeepThought\n","‚ú®üç∞‚ú® Everything looks OK!\n"]}]},{"cell_type":"code","source":["!conda install pip pytorch=2.5.1 torchvision=0.20.1 jupyter ipykernel torchmetrics"],"metadata":{"collapsed":true,"id":"WNpfiZpWU9cJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d9b47c0-cb3b-49ff-b0e1-2b9f99b68894"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Channels:\n"," - conda-forge\n","Platform: linux-64\n","Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ "]}]},{"cell_type":"code","source":["!pip install openimages ultralytics==8.3.40 opencv-python matplotlib Pillow requests scipy tqdm pandas seaborn tensorboard torchmetrics[detection] transformers==4.46.3"],"metadata":{"collapsed":true,"id":"Iqo-K4bMVI6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEL9vgidEmIV"},"outputs":[],"source":["# utils\n","from src.model_managers.standard_model_manager import (StandardModelManager,\n","                                                       FRCNNModelManager)\n","from tqdm import tqdm, tqdm_notebook\n","import matplotlib as plt\n","import pandas as pd\n","import numpy as np\n","import time\n","import os\n","\n","# torch\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import (fasterrcnn_resnet50_fpn_v2,\n","                                         fasterrcnn_resnet50_fpn)\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","from torchvision.transforms import v2\n","from torchvision import tv_tensors\n","import torch.optim as optim\n","import torch.nn as nn\n","import torchvision\n","import torch\n","\n","# transfomers\n","from transformers import BertTokenizer, BertForQuestionAnswering\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# load data\n","from src.dataset_loaders.download_openimages import (OpenImagesLoader,\n","                                                     ImageLoaderFRCNN)\n","from src.dataset_loaders.fruits360 import Fruits360Loader\n","\n","# set device\n","device = 'cpu'\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","elif torch.backends.mps.is_available():\n","    device = 'mps'\n","\n","print(f\"Device being used: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hHVMjbILEmId"},"outputs":[],"source":["# get data\n","# fl = Fruits360Loader(random_seed=101,\n","#                      batch_size=128,\n","#                      perc_keep=1.0)\n","# train_fl, val_fl, test_fl = fl.load_data()\n","\n","# oil = OpenImagesLoader(random_seed=101,\n","#                         batch_size=128,\n","#                         perc_keep=1.0,\n","#                         num_images_per_class=500,\n","#                        annotation_format='pascal')\n","\n","# oil.download_data()\n","# oil.split_data(keep_class_dirs=False)\n","#train, val, test = oil.get_datasets()\n","\n","\n","# csb = pd.read_csv(\"data/openimages_csv_dir/class-descriptions-boxable.csv\",\n","#                   header=None,\n","#                   names=['LabelName',\n","#                            'Label'])\n","# trabb = pd.read_csv(\"data/openimages_csv_dir/train-annotations-bbox.csv\",)\n","# vabb = pd.read_csv(\"data/openimages_csv_dir/validation-annotations-bbox.csv\",)\n","# teabb = pd.read_csv(\"data/openimages_csv_dir/test-annotations-bbox.csv\",)\n","\n","# food_cats = [\"Hot dog\", \"French fries\", \"Waffle\", \"Pancake\", \"Burrito\", \"Pretzel\",\n","#             \"Popcorn\", \"Cookie\", \"Muffin\", \"Ice cream\", \"Cake\", \"Candy\",\n","#             \"Guacamole\", \"Apple\", \"Grape\", \"Common fig\", \"Pear\",\n","#             \"Strawberry\", \"Tomato\", \"Lemon\", \"Banana\", \"Orange\", \"Peach\", \"Mango\",\n","#             \"Pineapple\", \"Grapefruit\", \"Pomegranate\", \"Watermelon\", \"Cantaloupe\",\n","#             \"Egg (Food)\", \"Bagel\", \"Bread\", \"Doughnut\", \"Croissant\",\n","#             \"Tart\", \"Mushroom\", \"Pasta\", \"Pizza\", \"Squid\",\n","#             \"Oyster\", \"Lobster\", \"Shrimp\", \"Crab\", \"Taco\", \"Cooking spray\",\n","#             \"Cucumber\", \"Radish\", \"Artichoke\", \"Potato\", \"Garden Asparagus\",\n","#             \"Pumpkin\", \"Zucchini\", \"Cabbage\", \"Carrot\", \"Salad\",\n","#             \"Broccoli\", \"Bell pepper\", \"Winter melon\", \"Honeycomb\",\n","#             \"Hamburger\", \"Submarine sandwich\", \"Cheese\", \"Milk\", \"Sushi\"]\n","\n","# csb = csb[csb['Label'].isin(food_cats)]\n","# csb\n","\n","# print(len(trabb))\n","# trabb = trabb[trabb['LabelName'].isin(csb[\"LabelName\"])]\n","# print(len(trabb))\n","# trabb_csb = pd.merge(csb, trabb, on='LabelName', how='inner')\n","# print(len(trabb_csb))\n","# trabb_csb\n","\n","# print(len(vabb))\n","# vabb = vabb[vabb['LabelName'].isin(csb[\"LabelName\"])]\n","# print(len(vabb))\n","# vabb_csb = pd.merge(csb, vabb, on='LabelName', how='inner')\n","# print(len(vabb_csb))\n","# vabb_csb\n","\n","# print(len(teabb))\n","# teabb = teabb[teabb['LabelName'].isin(csb[\"LabelName\"])]\n","# print(len(teabb))\n","# teabb_csb = pd.merge(csb, teabb, on='LabelName', how='inner')\n","# print(len(teabb_csb))\n","# teabb_csb\n","\n","# train_target = [{\"label\": torch.Tensor(row[\"Label\"]),\n","#                  \"boxes\": torch.tensor([row[\"XMin\"], row[\"XMax\"], row[\"YMin\"], row[\"YMax\"]])}\n","#                 for _, row in trabb_csb.iterrows()]\n","# train_target\n","\n","def get_transform(train):\n","    transf = []\n","    transf.append(v2.Resize((100)))\n","    transf.append(v2.ToTensor())\n","    if train:\n","        transf.append(v2.Normalize(mean=[0.485,\n","                                         0.456,\n","                                         0.406],\n","                                   std=[0.229,\n","                                        0.224,\n","                                        0.225]))\n","\n","    return v2.Compose(transf)\n","\n","ttform = get_transform(train=True)\n","vtform = get_transform(train=False)\n","\n","loader = OpenImagesLoader(random_seed=101,\n","                         batch_size=2,\n","                         perc_keep=1.0,\n","                         num_images_per_class=500,)\n","opim_dir = loader.data_dir\n","seed = loader.random_seed\n","batch_size = loader.batch_size\n","per_keep = loader.perc_keep\n","im_per_class = loader.num_images_per_class\n","\n","ann_form = loader.annotation_format\n","classes = loader.classes\n","class2index = loader.class_2_index\n","train_direct = loader.train_dir\n","val_direct = loader.val_dir\n","test_direct = loader.test_dir\n","\n","train_dataset = ImageLoaderFRCNN(root=train_direct,\n","                                 classes=classes,\n","                                 tforms=ttform)\n","val_dataset = ImageLoaderFRCNN(root=val_direct,\n","                               classes=classes,\n","                               tforms=vtform)\n","test_dataset = ImageLoaderFRCNN(root=test_direct,\n","                               classes=classes,)\n","\n","len(train_dataset), len(val_dataset), len(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2k3Nc-qEmIe"},"outputs":[],"source":["train_idx = list(range(1000))  # This will include indices 0 to 7500\n","val_idx = list(range(100))  # This will include indices 0 to 1000\n","test_idx = list(range(100))  # This will include indices 0 to 1000\n","tr_samp = SubsetRandomSampler(train_idx)\n","val_samp = SubsetRandomSampler(val_idx)\n","te_samp = SubsetRandomSampler(test_idx)\n","\n","def collate(data):\n","    return tuple(zip(*data))\n","\n","# without sampling lengths were train_loader: 98, val_loader: 13, test_loader: 13\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=False,\n","                                           num_workers=4,\n","                                           collate_fn=collate,\n","                                           sampler=tr_samp)\n","val_loader = torch.utils.data.DataLoader(val_dataset,\n","                                         batch_size=batch_size,\n","                                         shuffle=False,\n","                                         num_workers=4,\n","                                         collate_fn=collate,\n","                                         sampler=val_samp)\n","test_loader = torch.utils.data.DataLoader(test_dataset,\n","                                         batch_size=batch_size,\n","                                         shuffle=False,\n","                                         num_workers=4,\n","                                         collate_fn=collate,\n","                                         sampler=te_samp)\n","len(train_loader), len(val_loader), len(test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"do8QGVOJEmIf"},"outputs":[],"source":["# get/create model\n","def get_model(num_classes):\n","    # model types: fasterrcnn_resnet50_fpn,\n","    #              fasterrcnn_resnet50_fpn_v2,\n","    #              fasterrcnn_mobilenet_v3_large_fpn,\n","    #              fasterrcnn_mobilenet_v3_large_320_fpn\n","    model = fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n","\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model\n","\n","model = get_model(num_classes=138)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWGOjtEZEmIg"},"outputs":[],"source":["# train and evaluate model\n","lr = 0.001\n","epochs = 10\n","metric = MeanAveragePrecision()\n","optimizer = optim.AdamW(model.parameters(), lr=lr)\n","smmfr = FRCNNModelManager(model=model,\n","                         metric=metric,\n","                         optimizer=optimizer,\n","                         device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6DhEiYbEmIh","collapsed":true},"outputs":[],"source":["# import xml.etree.ElementTree as ET\n","# from PIL import Image\n","\n","# data_dir = os.path.join(\"data\", \"openimages\")\n","# train_dir = os.path.join(data_dir, \"train\") # Directory in which train dataset resides\n","# imgs = list(sorted(os.listdir(os.path.join(train_dir, \"images\"))))\n","# annotations = list(sorted(os.listdir(os.path.join(train_dir, \"annotations\"))))\n","# for i, c in enumerate(classes):\n","#     img_path = os.path.join(train_dir, \"images\", imgs[i])\n","#     ann_path = os.path.join(train_dir, \"annotations\", annotations[i])\n","\n","#     img = Image.open(img_path).convert(\"RGB\")\n","\n","#     # Parse the XML annotation file\n","#     tree = ET.parse(ann_path)\n","#     root = tree.getroot()\n","\n","#     boxes = []\n","#     labels = []\n","#     for obj in root.findall('object'):\n","#         label = obj.find('name').text.capitalize()\n","#         if \"food\" in label:\n","#             label = label.replace(\"food\", \"Food\")\n","#         print(label)\n","\n","\n","# fruits 360\n","# smm.train(training_data_loader=train_fl,\n","#           validation_data_loader=val,\n","#           epochs=epochs,\n","#           has_box=False)\n","\n","# google colab\n","# reduce batch size\n","# reduce image size\n","# smaller subset of data\n","\n","smmfr.train(training_data_loader=train_loader,\n","            validation_data_loader=val_loader,\n","            epochs=epochs,)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}