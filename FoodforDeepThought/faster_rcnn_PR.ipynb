{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3678,
     "status": "ok",
     "timestamp": 1733852555991,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "OLE73T11EpzF",
    "outputId": "3e744b7c-ce06-4a20-8d0f-09239d94db6f"
   },
   "outputs": [],
   "source": [
    "# # Mount into drive\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "# %cd '/content/drive/MyDrive/ColabNotebooks/gt_omscs_ml/deep_learning/Convolutionalists/FoodforDeepThought'\n",
    "\n",
    "# !pip install -q condacolab\n",
    "# import condacolab\n",
    "# condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 86314,
     "status": "ok",
     "timestamp": 1733852642303,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "WNpfiZpWU9cJ",
    "outputId": "5be1deee-5782-4854-fff4-eb69501f5fa6"
   },
   "outputs": [],
   "source": [
    "# !conda install pip pytorch=2.5.1 torchvision=0.20.1 jupyter ipykernel torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2980,
     "status": "ok",
     "timestamp": 1733852645278,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "Iqo-K4bMVI6N",
    "outputId": "62004489-8909-4195-8c82-29b2cc6f0bbb"
   },
   "outputs": [],
   "source": [
    "# !pip install openimages ultralytics==8.3.40 opencv-python matplotlib Pillow requests scipy tqdm pandas seaborn tensorboard torchmetrics[detection] transformers==4.46.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1733856742536,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "aEL9vgidEmIV",
    "outputId": "6048dff4-5f27-4d61-d4ad-7b758a09dff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "from src.model_managers.standard_model_manager import (StandardModelManager,\n",
    "                                                       FRCNNModelManager)\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import (fasterrcnn_resnet50_fpn_v2,\n",
    "                                         fasterrcnn_resnet50_fpn)\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# transfomers\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "\n",
    "# load data\n",
    "from src.dataset_loaders.download_openimages import OpenImagesLoader\n",
    "\n",
    "# set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "print(f\"Device being used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1733856745630,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "hHVMjbILEmId",
    "outputId": "b6393551-3fc6-4c4e-fb66-c5b49c74368d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12531, 1601, 1660)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_transform(train):\n",
    "    def resize_with_boxes(image, target):\n",
    "        orig_size = image.size\n",
    "        new_size = 100\n",
    "        image = v2.Resize((new_size, new_size))(image)\n",
    "        if target is not None:\n",
    "            scale_x, scale_y = new_size / orig_size[0], new_size / orig_size[1]\n",
    "            target[\"boxes\"][:, [0, 2]] *= scale_x\n",
    "            target[\"boxes\"][:, [1, 3]] *= scale_y\n",
    "        return image, target\n",
    "    #transf = []\n",
    "    #transf.append(v2.Resize((100)))\n",
    "    #transf.append(v2.ToTensor())\n",
    "    transf = [resize_with_boxes, v2.ToTensor()]\n",
    "    \n",
    "    if train:\n",
    "        transf.append(v2.Normalize(mean=[0.485,\n",
    "                                         0.456,\n",
    "                                         0.406],\n",
    "                                   std=[0.229,\n",
    "                                        0.224,\n",
    "                                        0.225]))\n",
    "\n",
    "    return v2.Compose(transf)\n",
    "\n",
    "ttform = get_transform(train=True)\n",
    "vtform = get_transform(train=False)\n",
    "\n",
    "loader = OpenImagesLoader(random_seed=101,\n",
    "                         batch_size=2,\n",
    "                         perc_keep=1.0,\n",
    "                         num_images_per_class=500,)\n",
    "opim_dir = loader.data_dir\n",
    "seed = loader.random_seed\n",
    "batch_size = loader.batch_size\n",
    "per_keep = loader.perc_keep\n",
    "im_per_class = loader.num_images_per_class\n",
    "\n",
    "ann_form = loader.annotation_format\n",
    "classes = loader.classes\n",
    "class2index = loader.class_2_index\n",
    "train_direct = loader.train_dir\n",
    "val_direct = loader.val_dir\n",
    "test_direct = loader.test_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRCNNDatasetProcessor():\n",
    "\n",
    "    def __init__(self, random_seed = 101, batch_size = 128, perc_keep = 1.0, num_images_per_class=500):\n",
    "        self.data_dir = os.path.join(\"data\", \"openimages\")  # Directory in which dataset resides\n",
    "        self.random_seed = random_seed\n",
    "        self.batch_size = batch_size\n",
    "        self.perc_keep = perc_keep  # Percentage of dataset to be kept (number between 0 and 1)\n",
    "        self.num_images_per_class = num_images_per_class\n",
    "\n",
    "        self.transforms_all = transforms.Compose(\n",
    "            [\n",
    "                Resize((256, 256)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.transforms_img = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet's normalization statistics\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classes = [\n",
    "            \"Hot dog\", \"French fries\", \"Waffle\", \"Pancake\", \"Burrito\", \"Pretzel\",\n",
    "            \"Popcorn\", \"Cookie\", \"Muffin\", \"Ice cream\", \"Cake\", \"Candy\",\n",
    "            \"Guacamole\", \"Apple\", \"Grape\", \"Common fig\", \"Pear\",\n",
    "            \"Strawberry\", \"Tomato\", \"Lemon\", \"Banana\", \"Orange\", \"Peach\", \"Mango\",\n",
    "            \"Pineapple\", \"Grapefruit\", \"Pomegranate\", \"Watermelon\", \"Cantaloupe\",\n",
    "            \"Egg (Food)\", \"Bagel\", \"Bread\", \"Doughnut\", \"Croissant\",\n",
    "            \"Tart\", \"Mushroom\", \"Pasta\", \"Pizza\", \"Squid\",\n",
    "            \"Oyster\", \"Lobster\", \"Shrimp\", \"Crab\", \"Taco\", \"Cooking spray\",\n",
    "            \"Cucumber\", \"Radish\", \"Artichoke\", \"Potato\", \"Garden Asparagus\",\n",
    "            \"Pumpkin\", \"Zucchini\", \"Cabbage\", \"Carrot\", \"Salad\",\n",
    "            \"Broccoli\", \"Bell pepper\", \"Winter melon\", \"Honeycomb\",\n",
    "            \"Hamburger\", \"Submarine sandwich\", \"Cheese\", \"Milk\", \"Sushi\"\n",
    "        ]\n",
    "\n",
    "        # Creating a dictionary mapping each class name to an index:\n",
    "        self.class_2_index = {}\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            self.class_2_index[class_name.lower()] = i\n",
    "\n",
    "        # Creating a dictionary mapping each class index to its corresponding class name:\n",
    "        self.index_2_class = {}\n",
    "        for k, v in self.class_2_index.items():\n",
    "            self.index_2_class[v] = k\n",
    "\n",
    "        self.train_dir = os.path.join(self.data_dir, \"train\") # Directory in which train dataset resides\n",
    "        self.val_dir = os.path.join(self.data_dir, \"val\") # Directory in which validation dataset resides\n",
    "        self.test_dir = os.path.join(self.data_dir, \"test\") # Directory in which test dataset resides\n",
    "\n",
    "        self.train_red_dir = os.path.join(self.data_dir, \"train_reduced\") # Directory in which reduced train dataset resides\n",
    "        self.val_red_dir = os.path.join(self.data_dir, \"val_reduced\") # Directory in which reduced validation dataset resides\n",
    "        self.test_red_dir = os.path.join(self.data_dir, \"test_reduced\") # Directory in which reduced test dataset resides\n",
    "\n",
    "\n",
    "    def split_data(self, keep_class_dirs=True):\n",
    "\n",
    "        \"\"\" This function splits the downloaded Open Image dataset, and splits each class into training, validation, and testing sets.\n",
    "            This function assumes that the required data has already been downloaded.\"\"\"\n",
    "\n",
    "        # Setting the random seed:\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "        splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        # Making folders for each of the splits:\n",
    "        for split in splits:\n",
    "            split_dir = os.path.join(self.data_dir, split)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        # Iterating through each class:\n",
    "        for class_cur in self.classes:\n",
    "            print(f'Splitting data for class {class_cur}')\n",
    "\n",
    "            # Getting directories for the images and annotations for each class:\n",
    "            imgs_dir = os.path.join(self.data_dir, class_cur.lower(), \"images\")\n",
    "            anns_dir = os.path.join(self.data_dir, class_cur.lower(), \"darknet\")\n",
    "\n",
    "            # Ensuring each class has images and annotations:\n",
    "            if not imgs_dir:\n",
    "                raise Exception(f'Images do not exist for {class_cur}!')\n",
    "\n",
    "            if not anns_dir:\n",
    "                raise Exception(f'Annotations do not exist for {class_cur}!')\n",
    "\n",
    "            class_imgs = os.listdir(imgs_dir) # Images for current class\n",
    "            class_anns = os.listdir(anns_dir) # Annotations for current class\n",
    "            class_imgs.sort()\n",
    "            class_anns.sort()\n",
    "\n",
    "            num_imgs = len(class_imgs) # Number of images and annotations for current class\n",
    "            \n",
    "            # Shuffling data:\n",
    "            inds_list = list(range(num_imgs)) # List of indices ranging for the total number of images\n",
    "            random.shuffle(inds_list) # Shuffling indices list\n",
    "            class_imgs = [class_imgs[i] for i in inds_list] # Shuffling class images according to shuffled inds_list\n",
    "            class_anns = [class_anns[i] for i in inds_list] # Shuffling class annotations according to shuffled inds_list\n",
    "\n",
    "            ind_train = int(0.8 * num_imgs) # Ending index for the training images\n",
    "            ind_val = ind_train + int(0.1 * num_imgs) # Ending index for the validation images\n",
    "\n",
    "            # Splitting images into training, validation, and testing:\n",
    "            train_imgs = class_imgs[:ind_train]\n",
    "            val_imgs = class_imgs[ind_train:ind_val]\n",
    "            test_imgs = class_imgs[ind_val:]\n",
    "\n",
    "            all_imgs = [train_imgs, val_imgs, test_imgs] # All images\n",
    "            \n",
    "            # Splitting annotations into training, validation, and testing:\n",
    "            train_anns = class_anns[:ind_train]\n",
    "            val_anns = class_anns[ind_train:ind_val]\n",
    "            test_anns = class_anns[ind_val:]\n",
    "\n",
    "            all_anns = [train_anns, val_anns, test_anns] # All annotations\n",
    "            \n",
    "            # Looping through all split types and corresponding split images:\n",
    "            for split_type, split_imgs, split_anns in zip(splits, all_imgs, all_anns):\n",
    "                if keep_class_dirs:\n",
    "                    # Creating each split directory for images and annotations for current class:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, class_cur.lower(), \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, class_cur.lower(), \"annotations\")\n",
    "                else:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, \"annotations\")\n",
    "\n",
    "                os.makedirs(split_dir_img, exist_ok=True)\n",
    "                os.makedirs(split_dir_ann, exist_ok=True)\n",
    "\n",
    "                # Copying each image from initial directory to corresponding split directory for each split:\n",
    "                for img, ann in zip(split_imgs, split_anns):\n",
    "                    shutil.copy(os.path.join(imgs_dir, img), os.path.join(split_dir_img, img))\n",
    "                    shutil.copy(os.path.join(anns_dir, ann), os.path.join(split_dir_ann, ann))\n",
    "\n",
    "                    # Code to replace each original class label (which is 0) to the class label as found in self.class_2_index:\n",
    "                    ann_file_cur_dir = os.path.join(split_dir_ann, ann) # File path of current annotation file\n",
    "                    with open(ann_file_cur_dir, 'r') as file:\n",
    "                        objects = file.readlines()\n",
    "\n",
    "                        new_labels = []\n",
    "                        for obj in objects:\n",
    "                            obj_items = obj.split()\n",
    "                            new_class_label = self.class_2_index[class_cur.lower()]\n",
    "                            obj_items[0] = str(new_class_label)\n",
    "\n",
    "                            obj_new = ' '.join(obj_items) + '\\n'\n",
    "                            new_labels.append(obj_new)\n",
    "                    \n",
    "                    with open(ann_file_cur_dir, 'w') as file:\n",
    "                        file.writelines(new_labels)\n",
    "                            \n",
    "\n",
    "\n",
    "    def make_dataloader(self, split_name):\n",
    "        \"\"\" Function to create a DataLoader object that's compatible with Facebook's DETR model.\n",
    "        \n",
    "        Inputs:\n",
    "        split_name (str) - must be one of the following: \"train\", \"train_reduced\", \"val\", \"val_reduced\", \"test\", \"test_reduced\n",
    "\n",
    "        Outputs:\n",
    "        dl (DataLoader) - DataLoader object\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def collate_fn(data):\n",
    "            \"\"\" Defining the collate function to pad and return images and list of annotations,\n",
    "                as object-detection can have variable image sizes and variable number of objects\n",
    "                in each image. \n",
    "            \"\"\"\n",
    "\n",
    "            # Extracting the images and corresponding annotations:\n",
    "            imgs, anns = zip(*data)\n",
    "\n",
    "            return torch.stack(imgs), anns        \n",
    "\n",
    "        imgs_dir = os.path.join(self.data_dir, split_name, \"images\")\n",
    "        anns_dir = os.path.join(self.data_dir, split_name, \"annotations\")\n",
    "\n",
    "        # Lists of all the images and corresponding annotation files in the selected directory:\n",
    "        imgs_list = os.listdir(imgs_dir)\n",
    "        anns_list = os.listdir(anns_dir)\n",
    "\n",
    "        dataset = []\n",
    "        # Iterating through each image and annotation pair:\n",
    "        for img_cur, ann_cur in zip(imgs_list, anns_list):\n",
    "            \n",
    "            # Directories of current image and annotation:\n",
    "            img_cur_dir = os.path.join(imgs_dir, img_cur)\n",
    "            ann_cur_dir = os.path.join(anns_dir, ann_cur)\n",
    "\n",
    "            # Reading image:\n",
    "            img_pil = Image.open(img_cur_dir).convert(\"RGB\")\n",
    "            img_size_orig = img_pil.size # (width, height) format\n",
    "            width, height = img_pil.size # Also getting width and height as separate variables\n",
    "            # img_tv = TVImage(torch.tensor(img_pil).permute(2, 0, 1))\n",
    "\n",
    "            ann_list = []\n",
    "            # Reading annotation file:\n",
    "            with open(ann_cur_dir, 'r') as file:\n",
    "\n",
    "                objects = file.readlines()\n",
    "\n",
    "                # Iterating through each object in the image (all assumed to equal to the target class):\n",
    "                for obj in objects:\n",
    "                    obj_items = obj.split()\n",
    "                    class_label = int(obj_items[0]) # Class label\n",
    "                    x_cent = float(obj_items[1]) * width # x-coordinate of bounding box's center\n",
    "                    y_cent = float(obj_items[2]) * height # y-coordinate of bounding box's center\n",
    "                    box_width = float(obj_items[3]) * width # Width of bounding box\n",
    "                    box_height = float(obj_items[4]) * height # Height of bounding box\n",
    "\n",
    "                    # Appending the bounding box information to the list of bounding box information:\n",
    "                    ann_list.append([x_cent, y_cent, box_width, box_height])\n",
    "\n",
    "                # Converting list of bounding box information to a PyTorch tensor:\n",
    "                box_tensor = torch.tensor(ann_list, dtype=torch.float)\n",
    "\n",
    "\n",
    "            # Bounding box object for current annotation file:\n",
    "            # bounding_boxes = tv_tensors.BoundingBoxes(box_tensor, format=\"CXCYWH\", canvas_size=img_size_orig)\n",
    "\n",
    "            box_tensor_convert = box_convert(box_tensor, in_fmt='cxcywh', out_fmt='xyxy') # Converting bounding boxes from CXCYWH format to XYXY format to make it compatible with DETR model\n",
    "            bounding_boxes = tv_tensors.BoundingBoxes(box_tensor_convert, format=\"XYXY\", canvas_size=img_size_orig)\n",
    "\n",
    "            # Applying transformations to bounding boxes and image:\n",
    "            img_trans = self.transforms_all(img_pil) # Applying the general transformations to the image\n",
    "            bb_trans = self.transforms_all(bounding_boxes) # Applying the general transformations to the image's corresponding bounding boxes\n",
    "            img_trans = self.transforms_img(img_trans) # Applying the image-specific transformations to the image (tensor conversion and normalization)\n",
    "\n",
    "            num_labels = bb_trans.shape[0] # Number of objects\n",
    "            labels_tensor = torch.ones(num_labels, dtype=torch.long) * class_label # Creating a labels tensor\n",
    "\n",
    "            ann_dict = {\"boxes\": bb_trans, \"class_labels\": labels_tensor}\n",
    "\n",
    "            info_tuple = (img_trans, ann_dict)\n",
    "            dataset.append(info_tuple)\n",
    "        \n",
    "\n",
    "        dataset_wrapper = CustomDataset(dataset)\n",
    "        dl = DataLoader(dataset_wrapper, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "        return dl\n",
    "\n",
    "\n",
    "class CustomDataset():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.dataset[ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frcnn_dp = FRCNNDatasetProcessor(batch_size = 16)\n",
    "\n",
    "train_dataset = frcnn_dp.make_dataloader(train_direct)\n",
    "val_dataset = frcnn_dp.make_dataloader(val_direct)\n",
    "test_dataset = frcnn_dp.make_dataloader(test_direct)\n",
    "\n",
    "\n",
    "# train_dataset = ImageLoaderFRCNN(root=train_direct,\n",
    "#                                  classes=classes,\n",
    "#                                  tforms=ttform)\n",
    "# val_dataset = ImageLoaderFRCNN(root=val_direct,\n",
    "#                                classes=classes,\n",
    "#                                tforms=vtform)\n",
    "# test_dataset = ImageLoaderFRCNN(root=test_direct,\n",
    "#                                classes=classes,)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733856745630,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "p2k3Nc-qEmIe",
    "outputId": "cba08b37-e5a8-445d-dacf-b6929dc5cc77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 50, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = list(range(len(train_dataset)))\n",
    "val_idx = list(range(len(val_dataset)))  \n",
    "test_idx = list(range(len(test_dataset)))  \n",
    "tr_samp = SubsetRandomSampler(train_idx)\n",
    "val_samp = SubsetRandomSampler(val_idx)\n",
    "te_samp = SubsetRandomSampler(test_idx)\n",
    "\n",
    "def collate(data):\n",
    "    return tuple(zip(*data))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate,\n",
    "                                           sampler=tr_samp)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate,\n",
    "                                         sampler=val_samp)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate,\n",
    "                                         sampler=te_samp)\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1234,
     "status": "ok",
     "timestamp": 1733856746861,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "do8QGVOJEmIf"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=\"COCO_V1\")\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(num_classes=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1733856746861,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "AWGOjtEZEmIg"
   },
   "outputs": [],
   "source": [
    "# train and evaluate model\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "metric = MeanAveragePrecision()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "smmfr = FRCNNModelManager(model=model,\n",
    "                         metric=metric,\n",
    "                         optimizer=optimizer,\n",
    "                         device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before transf img size: (1024, 1024)\n",
    "boxes size:\n",
    "tensor([[147., 243., 252., 339.],\n",
    "        [148., 336., 235., 427.],\n",
    "        [198., 385., 318., 460.],\n",
    "        [275., 230., 380., 324.],\n",
    "        [291.,  51., 425., 144.],\n",
    "        [316., 280., 519., 494.],\n",
    "        [344.,  97., 536., 291.],\n",
    "        [390., 702., 579., 903.],\n",
    "        [577., 265., 644., 366.],\n",
    "        [601., 356., 812., 556.],\n",
    "        [604., 155., 720., 284.],\n",
    "        [617., 934., 800., 971.],\n",
    "        [659., 284., 740., 372.],\n",
    "        [824., 307., 964., 494.],\n",
    "        [865., 716., 992., 961.]])\n",
    "\n",
    "\n",
    "After transf img size: torch.Size([3, 100, 100])\n",
    "boxes size:\n",
    "tensor([[14.3555, 23.7305, 24.6094, 33.1055],\n",
    "        [14.4531, 32.8125, 22.9492, 41.6992],\n",
    "        [19.3359, 37.5977, 31.0547, 44.9219],\n",
    "        [26.8555, 22.4609, 37.1094, 31.6406],\n",
    "        [28.4180,  4.9805, 41.5039, 14.0625],\n",
    "        [30.8594, 27.3438, 50.6836, 48.2422],\n",
    "        [33.5938,  9.4727, 52.3438, 28.4180],\n",
    "        [38.0859, 68.5547, 56.5430, 88.1836],\n",
    "        [56.3477, 25.8789, 62.8906, 35.7422],\n",
    "        [58.6914, 34.7656, 79.2969, 54.2969],\n",
    "        [58.9844, 15.1367, 70.3125, 27.7344],\n",
    "        [60.2539, 91.2109, 78.1250, 94.8242],\n",
    "        [64.3555, 27.7344, 72.2656, 36.3281],\n",
    "        [80.4688, 29.9805, 94.1406, 48.2422],\n",
    "        [84.4727, 69.9219, 96.8750, 93.8477]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1561552,
     "status": "error",
     "timestamp": 1733858309083,
     "user": {
      "displayName": "Scott Schmidl",
      "userId": "07500865660181256076"
     },
     "user_tz": 360
    },
    "id": "r6DhEiYbEmIh",
    "outputId": "cd51ea82-b01b-467c-989e-d11b6b88e21c"
   },
   "outputs": [],
   "source": [
    "smmfr.train(training_data_loader=train_loader,\n",
    "            validation_data_loader=val_loader,\n",
    "            epochs=epochs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJ2D0c9OsfHw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
