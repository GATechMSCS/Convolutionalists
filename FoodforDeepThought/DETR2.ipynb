{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27b52a-395a-4cf1-b771-2b46271ec61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from src.dataset_loaders.download_openimages import OpenImagesLoader\n",
    "from tqdm import tqdm, tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7f95ad-cf5c-4ded-a359-ac932338764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device Configuration:\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Device being used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3942cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from openimages.download import download_dataset\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image as PILImage\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.ops import box_convert\n",
    "from PIL import Image\n",
    "\n",
    "class DETRDatasetProcessor():\n",
    "\n",
    "    def __init__(self, random_seed = 101, batch_size = 128, perc_keep = 1.0, num_images_per_class=500):\n",
    "        self.data_dir = os.path.join(\"data\", \"openimages\")  # Directory in which dataset resides\n",
    "        self.random_seed = random_seed\n",
    "        self.batch_size = batch_size\n",
    "        self.perc_keep = perc_keep  # Percentage of dataset to be kept (number between 0 and 1)\n",
    "        self.num_images_per_class = num_images_per_class\n",
    "\n",
    "        self.transforms_all = transforms.Compose(\n",
    "            [\n",
    "                Resize((512, 512)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.transforms_img = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet's normalization statistics\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classes = [\n",
    "            \"Hot dog\", \"French fries\", \"Waffle\", \"Pancake\", \"Burrito\", \"Pretzel\",\n",
    "            \"Popcorn\", \"Cookie\", \"Muffin\", \"Ice cream\", \"Cake\", \"Candy\",\n",
    "            \"Guacamole\", \"Apple\", \"Grape\", \"Common fig\", \"Pear\",\n",
    "            \"Strawberry\", \"Tomato\", \"Lemon\", \"Banana\", \"Orange\", \"Peach\", \"Mango\",\n",
    "            \"Pineapple\", \"Grapefruit\", \"Pomegranate\", \"Watermelon\", \"Cantaloupe\",\n",
    "            \"Egg (Food)\", \"Bagel\", \"Bread\", \"Doughnut\", \"Croissant\",\n",
    "            \"Tart\", \"Mushroom\", \"Pasta\", \"Pizza\", \"Squid\",\n",
    "            \"Oyster\", \"Lobster\", \"Shrimp\", \"Crab\", \"Taco\", \"Cooking spray\",\n",
    "            \"Cucumber\", \"Radish\", \"Artichoke\", \"Potato\", \"Garden Asparagus\",\n",
    "            \"Pumpkin\", \"Zucchini\", \"Cabbage\", \"Carrot\", \"Salad\",\n",
    "            \"Broccoli\", \"Bell pepper\", \"Winter melon\", \"Honeycomb\",\n",
    "            \"Hamburger\", \"Submarine sandwich\", \"Cheese\", \"Milk\", \"Sushi\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        self.category_mapping = {i: {\"id\": i + 1, \"name\": name.lower()} for i, name in enumerate(self.classes)}\n",
    "        self.class_2_index = {entry[\"name\"]: entry[\"id\"] for entry in self.category_mapping.values()}\n",
    "\n",
    "        self.train_dir = os.path.join(self.data_dir, \"train\") # Directory in which train dataset resides\n",
    "        self.val_dir = os.path.join(self.data_dir, \"val\") # Directory in which validation dataset resides\n",
    "        self.test_dir = os.path.join(self.data_dir, \"test\") # Directory in which test dataset resides\n",
    "\n",
    "        self.train_red_dir = os.path.join(self.data_dir, \"train_red\") # Directory in which reduced train dataset resides\n",
    "        self.val_red_dir = os.path.join(self.data_dir, \"val_red\") # Directory in which reduced validation dataset resides\n",
    "        self.test_red_dir = os.path.join(self.data_dir, \"test_red\") # Directory in which reduced test dataset resides\n",
    "\n",
    "\n",
    "    def download_data(self, annotation_format='darknet'):\n",
    "        # download_dataset(self.data_dir, self.classes, annotation_format=annotation_format, limit=5)\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            print(f'Attempting to download {class_name} data')\n",
    "            if not os.path.isdir(os.path.join(self.data_dir, class_name.lower())):\n",
    "                try:\n",
    "                    download_dataset(self.data_dir, [class_name], annotation_format=annotation_format, limit=500)\n",
    "                except Exception as e:\n",
    "                    print(f'An exception occurred for {class_name}. ERROR: {e}')\n",
    "            else:\n",
    "                print(f'Skipped {class_name}, data already downloaded')\n",
    "\n",
    "\n",
    "    def split_data(self, keep_class_dirs=True):\n",
    "\n",
    "        \"\"\" This function splits the downloaded Open Image dataset, and splits each class into training, validation, and testing sets.\n",
    "            This function assumes that the required data has already been downloaded.\"\"\"\n",
    "\n",
    "        # Setting the random seed:\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "        splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        # Making folders for each of the splits:\n",
    "        for split in splits:\n",
    "            split_dir = os.path.join(self.data_dir, split)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        # Iterating through each class:\n",
    "        for class_cur in self.classes:\n",
    "            print(f'Splitting data for class {class_cur}')\n",
    "\n",
    "            # Getting directories for the images and annotations for each class:\n",
    "            imgs_dir = os.path.join(self.data_dir, class_cur.lower(), \"images\")\n",
    "            anns_dir = os.path.join(self.data_dir, class_cur.lower(), \"darknet\")\n",
    "\n",
    "            # Ensuring each class has images and annotations:\n",
    "            if not imgs_dir:\n",
    "                raise Exception(f'Images do not exist for {class_cur}!')\n",
    "\n",
    "            if not anns_dir:\n",
    "                raise Exception(f'Annotations do not exist for {class_cur}!')\n",
    "\n",
    "            class_imgs = os.listdir(imgs_dir) # Images for current class\n",
    "            class_anns = os.listdir(anns_dir) # Annotations for current class\n",
    "            class_imgs.sort()\n",
    "            class_anns.sort()\n",
    "\n",
    "            num_imgs = len(class_imgs) # Number of images and annotations for current class\n",
    "            \n",
    "            # Shuffling data:\n",
    "            inds_list = list(range(num_imgs)) # List of indices ranging for the total number of images\n",
    "            random.shuffle(inds_list) # Shuffling indices list\n",
    "            class_imgs = [class_imgs[i] for i in inds_list] # Shuffling class images according to shuffled inds_list\n",
    "            class_anns = [class_anns[i] for i in inds_list] # Shuffling class annotations according to shuffled inds_list\n",
    "\n",
    "            ind_train = int(0.8 * num_imgs) # Ending index for the training images\n",
    "            ind_val = ind_train + int(0.1 * num_imgs) # Ending index for the validation images\n",
    "\n",
    "            # Splitting images into training, validation, and testing:\n",
    "            train_imgs = class_imgs[:ind_train]\n",
    "            val_imgs = class_imgs[ind_train:ind_val]\n",
    "            test_imgs = class_imgs[ind_val:]\n",
    "\n",
    "            all_imgs = [train_imgs, val_imgs, test_imgs] # All images\n",
    "            \n",
    "            # Splitting annotations into training, validation, and testing:\n",
    "            train_anns = class_anns[:ind_train]\n",
    "            val_anns = class_anns[ind_train:ind_val]\n",
    "            test_anns = class_anns[ind_val:]\n",
    "\n",
    "            all_anns = [train_anns, val_anns, test_anns] # All annotations\n",
    "            \n",
    "            # Looping through all split types and corresponding split images:\n",
    "            for split_type, split_imgs, split_anns in zip(splits, all_imgs, all_anns):\n",
    "                if keep_class_dirs:\n",
    "                    # Creating each split directory for images and annotations for current class:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, class_cur.lower(), \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, class_cur.lower(), \"annotations\")\n",
    "                else:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, \"annotations\")\n",
    "\n",
    "                os.makedirs(split_dir_img, exist_ok=True)\n",
    "                os.makedirs(split_dir_ann, exist_ok=True)\n",
    "\n",
    "                # Copying each image from initial directory to corresponding split directory for each split:\n",
    "                for img, ann in zip(split_imgs, split_anns):\n",
    "                    shutil.copy(os.path.join(imgs_dir, img), os.path.join(split_dir_img, img))\n",
    "                    shutil.copy(os.path.join(anns_dir, ann), os.path.join(split_dir_ann, ann))\n",
    "\n",
    "                    # Code to replace each original class label (which is 0) to the class label as found in self.class_2_index:\n",
    "                    ann_file_cur_dir = os.path.join(split_dir_ann, ann) # File path of current annotation file\n",
    "                    with open(ann_file_cur_dir, 'r') as file:\n",
    "                        objects = file.readlines()\n",
    "\n",
    "                        new_labels = []\n",
    "                        for obj in objects:\n",
    "                            obj_items = obj.split()\n",
    "                            new_class_label = self.class_2_index[class_cur.lower()]\n",
    "                            obj_items[0] = str(new_class_label)\n",
    "\n",
    "                            obj_new = ' '.join(obj_items) + '\\n'\n",
    "                            new_labels.append(obj_new)\n",
    "                    \n",
    "                    with open(ann_file_cur_dir, 'w') as file:\n",
    "                        file.writelines(new_labels)\n",
    "                            \n",
    "\n",
    "    def split_data_reduced(self, keep_class_dirs=True):\n",
    "\n",
    "        \"\"\" This function splits the downloaded Open Image dataset, and splits each class into training, validation, and testing sets.\n",
    "            This function assumes that the required data has already been downloaded.\n",
    "            This function reduces the dataset by self.keep_perc. \"\"\"\n",
    "\n",
    "        # Setting the random seed:\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "        splits = [\"train_red\", \"val_red\", \"test_red\"]\n",
    "        \n",
    "        # Making folders for each of the splits:\n",
    "        for split in splits:\n",
    "            split_dir = os.path.join(self.data_dir, split)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        # Iterating through each class:\n",
    "        for class_cur in self.classes:\n",
    "            print(f'Splitting data for class {class_cur}')\n",
    "\n",
    "            # Getting directories for the images and annotations for each class:\n",
    "            imgs_dir = os.path.join(self.data_dir, class_cur.lower(), \"images\")\n",
    "            anns_dir = os.path.join(self.data_dir, class_cur.lower(), \"darknet\")\n",
    "\n",
    "            # Ensuring each class has images and annotations:\n",
    "            if not imgs_dir:\n",
    "                raise Exception(f'Images do not exist for {class_cur}!')\n",
    "\n",
    "            if not anns_dir:\n",
    "                raise Exception(f'Annotations do not exist for {class_cur}!')\n",
    "\n",
    "            class_imgs = os.listdir(imgs_dir) # Images for current class\n",
    "            class_anns = os.listdir(anns_dir) # Annotations for current class\n",
    "            class_imgs.sort()\n",
    "            class_anns.sort()\n",
    "\n",
    "            num_imgs = len(class_imgs) # Number of images and annotations for current class\n",
    "            \n",
    "            if self.perc_keep != 1.00 and num_imgs > 50:\n",
    "                num_imgs = int(num_imgs * self.perc_keep)\n",
    "                class_imgs = class_imgs[:num_imgs]\n",
    "                class_anns = class_anns[:num_imgs]\n",
    "\n",
    "            # Shuffling data:\n",
    "            inds_list = list(range(num_imgs)) # List of indices ranging for the total number of images\n",
    "            random.shuffle(inds_list) # Shuffling indices list\n",
    "            class_imgs = [class_imgs[i] for i in inds_list] # Shuffling class images according to shuffled inds_list\n",
    "            class_anns = [class_anns[i] for i in inds_list] # Shuffling class annotations according to shuffled inds_list\n",
    "\n",
    "            ind_train = int(0.8 * num_imgs) # Ending index for the training images\n",
    "            ind_val = ind_train + int(0.1 * num_imgs) # Ending index for the validation images\n",
    "\n",
    "            # Splitting images into training, validation, and testing:\n",
    "            train_imgs = class_imgs[:ind_train]\n",
    "            val_imgs = class_imgs[ind_train:ind_val]\n",
    "            test_imgs = class_imgs[ind_val:]\n",
    "\n",
    "            all_imgs = [train_imgs, val_imgs, test_imgs] # All images\n",
    "            \n",
    "            # Splitting annotations into training, validation, and testing:\n",
    "            train_anns = class_anns[:ind_train]\n",
    "            val_anns = class_anns[ind_train:ind_val]\n",
    "            test_anns = class_anns[ind_val:]\n",
    "\n",
    "            all_anns = [train_anns, val_anns, test_anns] # All annotations\n",
    "            \n",
    "            # Looping through all split types and corresponding split images:\n",
    "            for split_type, split_imgs, split_anns in zip(splits, all_imgs, all_anns):\n",
    "                if keep_class_dirs:\n",
    "                    # Creating each split directory for images and annotations for current class:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, class_cur.lower(), \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, class_cur.lower(), \"annotations\")\n",
    "                else:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, \"annotations\")\n",
    "\n",
    "                os.makedirs(split_dir_img, exist_ok=True)\n",
    "                os.makedirs(split_dir_ann, exist_ok=True)\n",
    "\n",
    "                # Copying each image from initial directory to corresponding split directory for each split:\n",
    "                for img, ann in zip(split_imgs, split_anns):\n",
    "                    shutil.copy(os.path.join(imgs_dir, img), os.path.join(split_dir_img, img))\n",
    "                    shutil.copy(os.path.join(anns_dir, ann), os.path.join(split_dir_ann, ann))\n",
    "\n",
    "                    # Code to replace each original class label (which is 0) to the class label as found in self.class_2_index:\n",
    "                    ann_file_cur_dir = os.path.join(split_dir_ann, ann) # File path of current annotation file\n",
    "                    with open(ann_file_cur_dir, 'r') as file:\n",
    "                        objects = file.readlines()\n",
    "\n",
    "                        new_labels = []\n",
    "                        for obj in objects:\n",
    "                            obj_items = obj.split()\n",
    "                            new_class_label = self.class_2_index[class_cur.lower()]\n",
    "                            obj_items[0] = str(new_class_label)\n",
    "\n",
    "                            obj_new = ' '.join(obj_items) + '\\n'\n",
    "                            new_labels.append(obj_new)\n",
    "                    \n",
    "                    with open(ann_file_cur_dir, 'w') as file:\n",
    "                        file.writelines(new_labels)\n",
    "\n",
    "        print(f\"Dataset has been reduced!\")\n",
    "\n",
    "\n",
    "\n",
    "    def convert_to_coco(self, reduced=True):\n",
    "        \"\"\" This function strives to convert the darknet annotations into COCO format. \"\"\"\n",
    "\n",
    "        # Split names:\n",
    "        if reduced:\n",
    "            splits = [\"train_red\", \"val_red\", \"test_red\"]\n",
    "        else:\n",
    "            splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        # Getting categories with category labels and corresponding id's:\n",
    "        categories = list(self.category_mapping.values())\n",
    "\n",
    "        # Initializing annotation and image id's:        \n",
    "        annotation_id = 1\n",
    "        image_id = 1\n",
    "        \n",
    "        # Iterating through all of the splits\n",
    "        for i, split in enumerate(splits):\n",
    "\n",
    "            coco_format = {\n",
    "                \"images\": [],\n",
    "                \"annotations\": [],\n",
    "                \"categories\": categories\n",
    "            }\n",
    "\n",
    "            img_folder = os.path.join(self.data_dir, split, \"images\")\n",
    "            ann_folder = os.path.join(self.data_dir, split, \"annotations\")\n",
    "\n",
    "            for img_file in os.listdir(img_folder):\n",
    "                \n",
    "                img_file_path = os.path.join(img_folder, img_file)\n",
    "                img = Image.open(img_file_path)\n",
    "                width, height = img.size\n",
    "                coco_format[\"images\"].append({\n",
    "                    \"id\": image_id,\n",
    "                    \"file_name\": img_file,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height\n",
    "                })\n",
    "\n",
    "                # Annotation:\n",
    "                ann_file_path = os.path.join(ann_folder, img_file.replace(\".jpg\", \".txt\"))                \n",
    "                with open(ann_file_path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        parts = line.strip().split()\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center, y_center, bbox_width, bbox_height = map(float, parts[1:])\n",
    "                        \n",
    "                        # Convert to COCO bbox format [x_min, y_min, width, height]:\n",
    "                        x_min = (x_center - bbox_width / 2) * width\n",
    "                        y_min = (y_center - bbox_height / 2) * height\n",
    "                        bbox_width *= width\n",
    "                        bbox_height *= height\n",
    "                        \n",
    "                        # Annotation information:\n",
    "                        coco_format[\"annotations\"].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": class_id,\n",
    "                            \"bbox\": [x_min, y_min, bbox_width, bbox_height],\n",
    "                            \"area\": bbox_width * bbox_height,\n",
    "                            \"iscrowd\": 0\n",
    "                        })\n",
    "\n",
    "                        # Incrementing the annotation id:\n",
    "                        annotation_id += 1\n",
    "                \n",
    "                # Incrementing the image id:\n",
    "                image_id += 1\n",
    "\n",
    "            # Setting the name of the .json file accordingly:\n",
    "            if i == 0:\n",
    "                with open(\"train_coco.json\", \"w\") as f:\n",
    "                    json.dump(coco_format, f, indent=4)                    \n",
    "            elif i == 1:\n",
    "                with open(\"val_coco.json\", \"w\") as f:\n",
    "                    json.dump(coco_format, f, indent=4)                    \n",
    "            else:\n",
    "                with open(\"test_coco.json\", \"w\") as f:\n",
    "                    json.dump(coco_format, f, indent=4)                    \n",
    "            \n",
    "            print(f\"Converted {split} to coco format!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4643233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Configuration & Hyperparameters:\n",
    "PERC_KEEP = 0.10 # Proportion of data from datasets to keep\n",
    "BATCH_SIZE = 16 # Batch size\n",
    "EPOCHS = 50 # Number of epochs to train the model for\n",
    "LEARNING_RATE = 1e-4 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49d0b78a-58d9-4169-99bd-3539ece25ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 64\n",
      "Converted train to coco format!\n",
      "Converted val to coco format!\n",
      "Converted test to coco format!\n"
     ]
    }
   ],
   "source": [
    "### Loading Open Images Dataset:\n",
    "\n",
    "# Initializing the DETRDatasetProcessor class:\n",
    "detr_processor = DETRDatasetProcessor(batch_size=BATCH_SIZE, perc_keep=PERC_KEEP)\n",
    "print(f\"Number of classes: {len(detr_processor.classes)}\")\n",
    "\n",
    "# Downloading the Open Images Dataset in darknet format:\n",
    "# detr_processor.download_data()\n",
    "\n",
    "# Splitting the downloaded data into training, validation, and test sets:\n",
    "# detr_processor.split_data(keep_class_dirs=False)\n",
    "\n",
    "# Splitting the downloaded data into reduced training, validation, and test sets:\n",
    "# detr_processor.split_data_reduced(keep_class_dirs=False)\n",
    "\n",
    "detr_processor.convert_to_coco(reduced=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69d0b6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 64, 'name': 'sushi'}\n"
     ]
    }
   ],
   "source": [
    "print(detr_processor.category_mapping[63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0093a-e4ad-479b-b1c6-a6da657e232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "git:\n",
      "  sha: N/A, status: clean, branch: N/A\n",
      "\n",
      "Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=150, lr_drop=200, clip_max_norm=0.1, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, masks=False, aux_loss=True, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, dataset_file='coco', coco_path='.\\\\data\\\\detr', coco_panoptic_path=None, remove_difficult=False, output_dir='.', device='cuda', seed=42, resume='', start_epoch=0, eval=False, num_workers=2, world_size=1, dist_url='env://', distributed=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\CS Grad School\\CS7643\\Final Project\\Convolutionalists\\FoodforDeepThought\\detr\\main.py:248\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_dir:\n\u001b[0;32m    247\u001b[0m     Path(args\u001b[38;5;241m.\u001b[39moutput_dir)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 248\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\CS Grad School\\CS7643\\Final Project\\Convolutionalists\\FoodforDeepThought\\detr\\main.py:121\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    118\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m    119\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m--> 121\u001b[0m model, criterion, postprocessors \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    124\u001b[0m model_without_ddp \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_detr_main\\models\\__init__.py:6\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(args):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_detr_main\\models\\detr.py:351\u001b[0m, in \u001b[0;36mbuild\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    348\u001b[0m     losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    349\u001b[0m criterion \u001b[38;5;241m=\u001b[39m SetCriterion(num_classes, matcher\u001b[38;5;241m=\u001b[39mmatcher, weight_dict\u001b[38;5;241m=\u001b[39mweight_dict,\n\u001b[0;32m    350\u001b[0m                          eos_coef\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39meos_coef, losses\u001b[38;5;241m=\u001b[39mlosses)\n\u001b[1;32m--> 351\u001b[0m \u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m postprocessors \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m: PostProcess()}\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmasks:\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:988\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 988\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Can also run this command in the terminal:\n",
    "%run detr/main.py --coco_path \".\\data\\detr\" --output_dir \".\" --epochs 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36942380-5c19-4213-8d50-d40dec1a1aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\prave/.cache\\torch\\hub\\facebookresearch_detr_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified model saved to modified_detr.pth\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-trained DETR R50 model:\n",
    "checkpoint_path = \"detr-r50-e632da11.pth\"  # Path to the original checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "# Removing the class_embed weights from the checkpoint:\n",
    "checkpoint[\"model\"] = {k: v for k, v in checkpoint[\"model\"].items() if not k.startswith(\"class_embed\")}\n",
    "\n",
    "# Loading the pretrained DETR model from PyTorch Hub to have a base model object to work off of:\n",
    "detr_model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
    "\n",
    "# Replacing the classification head to match the number of custom classes:\n",
    "num_labels = 66\n",
    "detr_model.class_embed = nn.Linear(detr_model.class_embed.in_features, num_labels)\n",
    "\n",
    "# Initializing new classification head:\n",
    "nn.init.xavier_uniform_(detr_model.class_embed.weight)\n",
    "nn.init.constant_(detr_model.class_embed.bias, 0)\n",
    "\n",
    "# Loading the modified checkpoint into the model:\n",
    "detr_model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "# Saving the modified model's state dictionary:\n",
    "model_path = \"modified_detr.pth\"\n",
    "torch.save({'model': detr_model.state_dict()}, model_path)\n",
    "\n",
    "print(f\"Modified model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d8705-c919-4e09-9b92-8e73933fce32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
