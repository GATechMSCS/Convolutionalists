{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27b52a-395a-4cf1-b771-2b46271ec61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "# from src.model_managers.detr_model_manager import DETRModelManager\n",
    "from src.dataset_loaders.download_openimages import OpenImagesLoader\n",
    "# from src.dataset_loaders.detr_dataset_processor import DETRDatasetProcessor\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "from transformers import DetrForObjectDetection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f95ad-cf5c-4ded-a359-ac932338764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device Configuration:\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Device being used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3942cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from openimages.download import download_dataset\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image as PILImage\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "\n",
    "class DETRDatasetProcessor():\n",
    "\n",
    "    def __init__(self, random_seed = 101, batch_size = 128, perc_keep = 1.0, num_images_per_class=500):\n",
    "        self.data_dir = os.path.join(\"data\", \"openimages\")  # Directory in which dataset resides\n",
    "        self.random_seed = random_seed\n",
    "        self.batch_size = batch_size\n",
    "        self.perc_keep = perc_keep  # Percentage of dataset to be kept (number between 0 and 1)\n",
    "        self.num_images_per_class = num_images_per_class\n",
    "\n",
    "        self.transforms_all = transforms.Compose(\n",
    "            [\n",
    "                Resize((512, 512)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.transforms_img = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet's normalization statistics\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classes = [\n",
    "            \"Hot dog\", \"French fries\", \"Waffle\", \"Pancake\", \"Burrito\", \"Pretzel\",\n",
    "            \"Popcorn\", \"Cookie\", \"Muffin\", \"Ice cream\", \"Cake\", \"Candy\",\n",
    "            \"Guacamole\", \"Apple\", \"Grape\", \"Common fig\", \"Pear\",\n",
    "            \"Strawberry\", \"Tomato\", \"Lemon\", \"Banana\", \"Orange\", \"Peach\", \"Mango\",\n",
    "            \"Pineapple\", \"Grapefruit\", \"Pomegranate\", \"Watermelon\", \"Cantaloupe\",\n",
    "            \"Egg (Food)\", \"Bagel\", \"Bread\", \"Doughnut\", \"Croissant\",\n",
    "            \"Tart\", \"Mushroom\", \"Pasta\", \"Pizza\", \"Squid\",\n",
    "            \"Oyster\", \"Lobster\", \"Shrimp\", \"Crab\", \"Taco\", \"Cooking spray\",\n",
    "            \"Cucumber\", \"Radish\", \"Artichoke\", \"Potato\", \"Garden Asparagus\",\n",
    "            \"Pumpkin\", \"Zucchini\", \"Cabbage\", \"Carrot\", \"Salad\",\n",
    "            \"Broccoli\", \"Bell pepper\", \"Winter melon\", \"Honeycomb\",\n",
    "            \"Hamburger\", \"Submarine sandwich\", \"Cheese\", \"Milk\", \"Sushi\"\n",
    "        ]\n",
    "\n",
    "        # Creating a dictionary mapping each class name to an index:\n",
    "        self.class_2_index = {}\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            self.class_2_index[class_name.lower()] = i\n",
    "\n",
    "        # Creating a dictionary mapping each class index to its corresponding class name:\n",
    "        self.index_2_class = {}\n",
    "        for k, v in self.class_2_index.items():\n",
    "            self.index_2_class[v] = k\n",
    "\n",
    "        self.train_dir = os.path.join(self.data_dir, \"train\") # Directory in which train dataset resides\n",
    "        self.val_dir = os.path.join(self.data_dir, \"val\") # Directory in which validation dataset resides\n",
    "        self.test_dir = os.path.join(self.data_dir, \"test\") # Directory in which test dataset resides\n",
    "\n",
    "        self.train_red_dir = os.path.join(self.data_dir, \"train_reduced\") # Directory in which reduced train dataset resides\n",
    "        self.val_red_dir = os.path.join(self.data_dir, \"val_reduced\") # Directory in which reduced validation dataset resides\n",
    "        self.test_red_dir = os.path.join(self.data_dir, \"test_reduced\") # Directory in which reduced test dataset resides\n",
    "\n",
    "\n",
    "    def download_data(self, annotation_format='darknet'):\n",
    "        # download_dataset(self.data_dir, self.classes, annotation_format=annotation_format, limit=5)\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            print(f'Attempting to download {class_name} data')\n",
    "            if not os.path.isdir(os.path.join(self.data_dir, class_name.lower())):\n",
    "                try:\n",
    "                    download_dataset(self.data_dir, [class_name], annotation_format=annotation_format, limit=5)\n",
    "                except Exception as e:\n",
    "                    print(f'An exception occurred for {class_name}. ERROR: {e}')\n",
    "            else:\n",
    "                print(f'Skipped {class_name}, data already downloaded')\n",
    "\n",
    "\n",
    "    def split_data(self, keep_class_dirs=True):\n",
    "\n",
    "        \"\"\" This function splits the downloaded Open Image dataset, and splits each class into training, validation, and testing sets.\n",
    "            This function assumes that the required data has already been downloaded.\"\"\"\n",
    "\n",
    "        # Setting the random seed:\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "        splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        # Making folders for each of the splits:\n",
    "        for split in splits:\n",
    "            split_dir = os.path.join(self.data_dir, split)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        # Iterating through each class:\n",
    "        for class_cur in self.classes:\n",
    "            print(f'Splitting data for class {class_cur}')\n",
    "\n",
    "            # Getting directories for the images and annotations for each class:\n",
    "            imgs_dir = os.path.join(self.data_dir, class_cur.lower(), \"images\")\n",
    "            anns_dir = os.path.join(self.data_dir, class_cur.lower(), \"darknet\")\n",
    "\n",
    "            # Ensuring each class has images and annotations:\n",
    "            if not imgs_dir:\n",
    "                raise Exception(f'Images do not exist for {class_cur}!')\n",
    "\n",
    "            if not anns_dir:\n",
    "                raise Exception(f'Annotations do not exist for {class_cur}!')\n",
    "\n",
    "            class_imgs = os.listdir(imgs_dir) # Images for current class\n",
    "            class_anns = os.listdir(anns_dir) # Annotations for current class\n",
    "            class_imgs.sort()\n",
    "            class_anns.sort()\n",
    "\n",
    "            num_imgs = len(class_imgs) # Number of images and annotations for current class\n",
    "            \n",
    "            # Shuffling data:\n",
    "            inds_list = list(range(num_imgs)) # List of indices ranging for the total number of images\n",
    "            random.shuffle(inds_list) # Shuffling indices list\n",
    "            class_imgs = [class_imgs[i] for i in inds_list] # Shuffling class images according to shuffled inds_list\n",
    "            class_anns = [class_anns[i] for i in inds_list] # Shuffling class annotations according to shuffled inds_list\n",
    "\n",
    "            ind_train = int(0.8 * num_imgs) # Ending index for the training images\n",
    "            ind_val = ind_train + int(0.1 * num_imgs) # Ending index for the validation images\n",
    "\n",
    "            # Splitting images into training, validation, and testing:\n",
    "            train_imgs = class_imgs[:ind_train]\n",
    "            val_imgs = class_imgs[ind_train:ind_val]\n",
    "            test_imgs = class_imgs[ind_val:]\n",
    "\n",
    "            all_imgs = [train_imgs, val_imgs, test_imgs] # All images\n",
    "            \n",
    "            # Splitting annotations into training, validation, and testing:\n",
    "            train_anns = class_anns[:ind_train]\n",
    "            val_anns = class_anns[ind_train:ind_val]\n",
    "            test_anns = class_anns[ind_val:]\n",
    "\n",
    "            all_anns = [train_anns, val_anns, test_anns] # All annotations\n",
    "            \n",
    "            # Looping through all split types and corresponding split images:\n",
    "            for split_type, split_imgs, split_anns in zip(splits, all_imgs, all_anns):\n",
    "                if keep_class_dirs:\n",
    "                    # Creating each split directory for images and annotations for current class:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, class_cur.lower(), \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, class_cur.lower(), \"annotations\")\n",
    "                else:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, \"annotations\")\n",
    "\n",
    "                os.makedirs(split_dir_img, exist_ok=True)\n",
    "                os.makedirs(split_dir_ann, exist_ok=True)\n",
    "\n",
    "                # Copying each image from initial directory to corresponding split directory for each split:\n",
    "                for img, ann in zip(split_imgs, split_anns):\n",
    "                    shutil.copy(os.path.join(imgs_dir, img), os.path.join(split_dir_img, img))\n",
    "                    shutil.copy(os.path.join(anns_dir, ann), os.path.join(split_dir_ann, ann))\n",
    "\n",
    "                    # Code to replace each original class label (which is 0) to the class label as found in self.class_2_index:\n",
    "                    ann_file_cur_dir = os.path.join(split_dir_ann, ann) # File path of current annotation file\n",
    "                    with open(ann_file_cur_dir, 'r') as file:\n",
    "                        objects = file.readlines()\n",
    "\n",
    "                        new_labels = []\n",
    "                        for obj in objects:\n",
    "                            obj_items = obj.split()\n",
    "                            new_class_label = self.class_2_index[class_cur.lower()]\n",
    "                            obj_items[0] = str(new_class_label)\n",
    "\n",
    "                            obj_new = ' '.join(obj_items) + '\\n'\n",
    "                            new_labels.append(obj_new)\n",
    "                    \n",
    "                    with open(ann_file_cur_dir, 'w') as file:\n",
    "                        file.writelines(new_labels)\n",
    "                            \n",
    "\n",
    "    def split_data_reduced(self, keep_class_dirs=True):\n",
    "\n",
    "        \"\"\" This function splits the downloaded Open Image dataset, and splits each class into training, validation, and testing sets.\n",
    "            This function assumes that the required data has already been downloaded.\n",
    "            This function reduces the dataset by self.keep_perc. \"\"\"\n",
    "\n",
    "        # Setting the random seed:\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "        splits = [\"train_reduced\", \"val_reduced\", \"test_reduced\"]\n",
    "        \n",
    "        # Making folders for each of the splits:\n",
    "        for split in splits:\n",
    "            split_dir = os.path.join(self.data_dir, split)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        # Iterating through each class:\n",
    "        for class_cur in self.classes:\n",
    "            print(f'Splitting data for class {class_cur}')\n",
    "\n",
    "            # Getting directories for the images and annotations for each class:\n",
    "            imgs_dir = os.path.join(self.data_dir, class_cur.lower(), \"images\")\n",
    "            anns_dir = os.path.join(self.data_dir, class_cur.lower(), \"darknet\")\n",
    "\n",
    "            # Ensuring each class has images and annotations:\n",
    "            if not imgs_dir:\n",
    "                raise Exception(f'Images do not exist for {class_cur}!')\n",
    "\n",
    "            if not anns_dir:\n",
    "                raise Exception(f'Annotations do not exist for {class_cur}!')\n",
    "\n",
    "            class_imgs = os.listdir(imgs_dir) # Images for current class\n",
    "            class_anns = os.listdir(anns_dir) # Annotations for current class\n",
    "            class_imgs.sort()\n",
    "            class_anns.sort()\n",
    "\n",
    "            num_imgs = len(class_imgs) # Number of images and annotations for current class\n",
    "            \n",
    "            if self.perc_keep != 1.00 and num_imgs > 50:\n",
    "                num_imgs = int(num_imgs * self.perc_keep)\n",
    "                class_imgs = class_imgs[:num_imgs]\n",
    "                class_anns = class_anns[:num_imgs]\n",
    "\n",
    "            # Shuffling data:\n",
    "            inds_list = list(range(num_imgs)) # List of indices ranging for the total number of images\n",
    "            random.shuffle(inds_list) # Shuffling indices list\n",
    "            class_imgs = [class_imgs[i] for i in inds_list] # Shuffling class images according to shuffled inds_list\n",
    "            class_anns = [class_anns[i] for i in inds_list] # Shuffling class annotations according to shuffled inds_list\n",
    "\n",
    "            ind_train = int(0.8 * num_imgs) # Ending index for the training images\n",
    "            ind_val = ind_train + int(0.1 * num_imgs) # Ending index for the validation images\n",
    "\n",
    "            # Splitting images into training, validation, and testing:\n",
    "            train_imgs = class_imgs[:ind_train]\n",
    "            val_imgs = class_imgs[ind_train:ind_val]\n",
    "            test_imgs = class_imgs[ind_val:]\n",
    "\n",
    "            all_imgs = [train_imgs, val_imgs, test_imgs] # All images\n",
    "            \n",
    "            # Splitting annotations into training, validation, and testing:\n",
    "            train_anns = class_anns[:ind_train]\n",
    "            val_anns = class_anns[ind_train:ind_val]\n",
    "            test_anns = class_anns[ind_val:]\n",
    "\n",
    "            all_anns = [train_anns, val_anns, test_anns] # All annotations\n",
    "            \n",
    "            # Looping through all split types and corresponding split images:\n",
    "            for split_type, split_imgs, split_anns in zip(splits, all_imgs, all_anns):\n",
    "                if keep_class_dirs:\n",
    "                    # Creating each split directory for images and annotations for current class:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, class_cur.lower(), \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, class_cur.lower(), \"annotations\")\n",
    "                else:\n",
    "                    split_dir_img = os.path.join(self.data_dir, split_type, \"images\")\n",
    "                    split_dir_ann = os.path.join(self.data_dir, split_type, \"annotations\")\n",
    "\n",
    "                os.makedirs(split_dir_img, exist_ok=True)\n",
    "                os.makedirs(split_dir_ann, exist_ok=True)\n",
    "\n",
    "                # Copying each image from initial directory to corresponding split directory for each split:\n",
    "                for img, ann in zip(split_imgs, split_anns):\n",
    "                    shutil.copy(os.path.join(imgs_dir, img), os.path.join(split_dir_img, img))\n",
    "                    shutil.copy(os.path.join(anns_dir, ann), os.path.join(split_dir_ann, ann))\n",
    "\n",
    "                    # Code to replace each original class label (which is 0) to the class label as found in self.class_2_index:\n",
    "                    ann_file_cur_dir = os.path.join(split_dir_ann, ann) # File path of current annotation file\n",
    "                    with open(ann_file_cur_dir, 'r') as file:\n",
    "                        objects = file.readlines()\n",
    "\n",
    "                        new_labels = []\n",
    "                        for obj in objects:\n",
    "                            obj_items = obj.split()\n",
    "                            new_class_label = self.class_2_index[class_cur.lower()]\n",
    "                            obj_items[0] = str(new_class_label)\n",
    "\n",
    "                            obj_new = ' '.join(obj_items) + '\\n'\n",
    "                            new_labels.append(obj_new)\n",
    "                    \n",
    "                    with open(ann_file_cur_dir, 'w') as file:\n",
    "                        file.writelines(new_labels)\n",
    "\n",
    "        print(f\"Dataset has been reduced!\")\n",
    "\n",
    "\n",
    "    def see_class_labels(self):\n",
    "        \"\"\" Simple function to determine if all of the images have just one class in them. \"\"\"\n",
    "\n",
    "        # Change the split to \"train\", \"val\", or \"test\" to choose which directory you need to test:\n",
    "        anns_dir =  os.path.join(self.data_dir, \"test\", \"annotations\")\n",
    "\n",
    "        ann_files = os.listdir(anns_dir)\n",
    "\n",
    "        for ann_file in ann_files:\n",
    "            ann_file_dir = os.path.join(anns_dir, ann_file)\n",
    "\n",
    "            with open(ann_file_dir, 'r') as file:\n",
    "                objects = file.readlines()\n",
    "\n",
    "                for obj in objects:\n",
    "                    class_label = int(obj.split()[0])\n",
    "                    if class_label != 0:\n",
    "                        print(f\"Object {obj} in file {ann_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def make_dataloader(self, split_name):\n",
    "        \"\"\" Function to create a DataLoader object that's compatible with Facebook's DETR model.\n",
    "        \n",
    "        Inputs:\n",
    "        split_name (str) - must be one of the following: \"train\", \"train_reduced\", \"val\", \"val_reduced\", \"test\", \"test_reduced\n",
    "\n",
    "        Outputs:\n",
    "        dl (DataLoader) - DataLoader object\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def collate_fn(data):\n",
    "            \"\"\" Defining the collate function to pad and return images and list of annotations,\n",
    "                as object-detection can have variable image sizes and variable number of objects\n",
    "                in each image. \n",
    "            \"\"\"\n",
    "\n",
    "            # Extracting the images and corresponding annotations:\n",
    "            imgs, anns = zip(*data)\n",
    "\n",
    "            return torch.stack(imgs), anns        \n",
    "\n",
    "        imgs_dir = os.path.join(self.data_dir, split_name, \"images\")\n",
    "        anns_dir = os.path.join(self.data_dir, split_name, \"annotations\")\n",
    "\n",
    "        # Lists of all the images and corresponding annotation files in the selected directory:\n",
    "        imgs_list = os.listdir(imgs_dir)\n",
    "        anns_list = os.listdir(anns_dir)\n",
    "\n",
    "        dataset = []\n",
    "        # Iterating through each image and annotation pair:\n",
    "        for img_cur, ann_cur in zip(imgs_list, anns_list):\n",
    "            \n",
    "            # Directories of current image and annotation:\n",
    "            img_cur_dir = os.path.join(imgs_dir, img_cur)\n",
    "            ann_cur_dir = os.path.join(anns_dir, ann_cur)\n",
    "\n",
    "            # Reading image:\n",
    "            img_pil = PILImage.open(img_cur_dir).convert(\"RGB\")\n",
    "            img_size_orig = img_pil.size # (width, height) format\n",
    "            # img_tv = TVImage(torch.tensor(img_pil).permute(2, 0, 1))\n",
    "\n",
    "            ann_list = []\n",
    "            # Reading annotation file:\n",
    "            with open(ann_cur_dir, 'r') as file:\n",
    "\n",
    "                objects = file.readlines()\n",
    "\n",
    "                # Iterating through each object in the image (all assumed to equal to the target class):\n",
    "                for obj in objects:\n",
    "                    obj_items = obj.split()\n",
    "                    class_label = int(obj_items[0]) # Class label\n",
    "                    x_cent = float(obj_items[1]) # x-coordinate of bounding box's center\n",
    "                    y_cent = float(obj_items[2]) # y-coordinate of bounding box's center\n",
    "                    box_width = float(obj_items[3]) # Width of bounding box\n",
    "                    box_height = float(obj_items[4]) # Height of bounding box\n",
    "\n",
    "                    # Appending the bounding box information to the list of bounding box information:\n",
    "                    ann_list.append([x_cent, y_cent, box_width, box_height])\n",
    "\n",
    "                # Converting list of bounding box information to a PyTorch tensor:\n",
    "                box_tensor = torch.tensor(ann_list, dtype=torch.float)\n",
    "\n",
    "            box_tensor_convert = box_convert(box_tensor, in_fmt='cxcywh', out_fmt='xyxy') # Converting bounding boxes from CXCYWH format to XYXY format to make it compatible with DETR model\n",
    "\n",
    "            # Bounding box object for current annotation file:\n",
    "            bounding_boxes = tv_tensors.BoundingBoxes(box_tensor_convert, format=\"XYXY\", canvas_size=img_size_orig)\n",
    "\n",
    "            # Applying transformations to bounding boxes and image:\n",
    "            img_trans = self.transforms_all(img_pil) # Applying the general transformations to the image\n",
    "            bb_trans = self.transforms_all(bounding_boxes) # Applying the general transformations to the image's corresponding bounding boxes\n",
    "            img_trans = self.transforms_img(img_trans) # Applying the image-specific transformations to the image (tensor conversion and normalization)\n",
    "\n",
    "            num_labels = bb_trans.shape[0] # Number of objects\n",
    "            labels_tensor = torch.ones(num_labels, dtype=torch.int64) * class_label # Creating a labels tensor\n",
    "\n",
    "            ann_dict = {\"boxes\": bb_trans, \"class_labels\": labels_tensor}\n",
    "\n",
    "            info_tuple = (img_trans, ann_dict)\n",
    "            dataset.append(info_tuple)\n",
    "        \n",
    "\n",
    "        dataset_wrapper = DETRDataset(dataset)\n",
    "        dl = DataLoader(dataset_wrapper, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "        return dl\n",
    "\n",
    "\n",
    "class DETRDataset():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.dataset[ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Configuration & Hyperparameters:\n",
    "PERC_KEEP = 1.00 # Proportion of data from datasets to keep\n",
    "BATCH_SIZE = 16 # Batch size\n",
    "EPOCHS = 50 # Number of epochs to train the model for\n",
    "LEARNING_RATE = 1e-5 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0b78a-58d9-4169-99bd-3539ece25ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Open Images Dataset:\n",
    "\n",
    "# Initializing the DETRDatasetProcessor class:\n",
    "detr_processor = DETRDatasetProcessor(batch_size=BATCH_SIZE, perc_keep=PERC_KEEP)\n",
    "print(f\"Number of classes: {len(detr_processor.classes)}\")\n",
    "\n",
    "# Downloading the Open Images Dataset in darknet format:\n",
    "# detr_processor.download_data()\n",
    "\n",
    "# Splitting the downloaded data into training, validation, and test sets:\n",
    "# detr_processor.split_data(keep_class_dirs=False)\n",
    "\n",
    "# Splitting the downloaded data into reduced training, validation, and test sets:\n",
    "# detr_processor.split_data_reduced(keep_class_dirs=False)\n",
    "\n",
    "# Creating training, validation, and testing dataloaders:\n",
    "train_set = detr_processor.make_dataloader(\"train_reduced\")\n",
    "val_set = detr_processor.make_dataloader(\"val_reduced\")\n",
    "test_set = detr_processor.make_dataloader(\"test_reduced\")\n",
    "\n",
    "print(f\"Number of Batches in Training Set: {len(train_set)}\")\n",
    "print(f\"Number of Batches in Validation Set: {len(val_set)}\")\n",
    "print(f\"Number of Batches in Testing Set: {len(test_set)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torchvision\n",
    "\n",
    "class DETRModelManager:\n",
    "    def __init__(self, model, optimizer, device=None):\n",
    "\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.confidence_threshold = 0.01\n",
    "        self.best_accuracy = 0.0\n",
    "        self.best_loss = np.inf\n",
    "        self.best_model_state_dict = None\n",
    "        \n",
    "        self.training_losses = None\n",
    "        self.val_losses = None\n",
    "        self.training_maps = None\n",
    "        self.val_maps = None\n",
    "        self.training_maps_50 = None\n",
    "        self.val_maps_50 = None\n",
    "\n",
    "    def train(self, training_data_loader, validation_data_loader = None, epochs=10):\n",
    "\n",
    "        # Creating empty lists for the training and validation losses and mAP values:\n",
    "        training_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        training_maps = []\n",
    "        val_maps = []\n",
    "\n",
    "        training_maps_50 = []\n",
    "        val_maps_50 = []\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            display_epoch = epoch + 1\n",
    "\n",
    "            self.model.train() # Putting the model (back) in training mode\n",
    "            \n",
    "            loss_epoch = 0\n",
    "            map_epoch = MeanAveragePrecision()\n",
    "\n",
    "\n",
    "            for idx, (imgs, anns) in enumerate(training_data_loader):\n",
    "                \n",
    "                # Extracting height and width of each image in the batch as well as batch size:\n",
    "                batch_size = imgs.shape[0]\n",
    "                img_height = 512\n",
    "                img_width = 512\n",
    "                \n",
    "                # Moving all input and target tensors to device:\n",
    "                inputs = {\"pixel_values\": imgs.to(self.device)} # Creating inputs for model\n",
    "                anns = [{key: val.to(self.device) for key, val in ann.items()} for ann in anns]\n",
    "\n",
    "                # Forward pass:\n",
    "                output = self.model(**inputs, labels=anns)\n",
    "\n",
    "                # Loss from current batch:\n",
    "                loss = output.loss\n",
    "                loss_epoch += loss\n",
    "\n",
    "                # Backward pass:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                ### Check MAP:\n",
    "\n",
    "                # Getting predictions:\n",
    "                pred = []\n",
    "                for ind in range(batch_size):\n",
    "\n",
    "                    # Extracting normalized bounding boxes:\n",
    "                    pred_boxes = output.pred_boxes[ind]  # Normalized [center_x, center_y, width, height]\n",
    "                    pred_scores = torch.max(torch.softmax(output.logits[ind], dim=-1), dim=-1).values\n",
    "                    pred_labels = torch.argmax(output.logits[ind], dim=-1)\n",
    "\n",
    "                    # Applying confidence threshold filtering:\n",
    "                    valid_indices = pred_scores > self.confidence_threshold\n",
    "                    pred_boxes = pred_boxes[valid_indices]\n",
    "                    pred_scores = pred_scores[valid_indices]\n",
    "                    pred_labels = pred_labels[valid_indices]\n",
    "\n",
    "                    # Denormalizing bounding boxes:\n",
    "                    pred_boxes[:, 0] *= img_width  # center_x\n",
    "                    pred_boxes[:, 1] *= img_height  # center_y\n",
    "                    pred_boxes[:, 2] *= img_width  # width\n",
    "                    pred_boxes[:, 3] *= img_height  # height\n",
    "\n",
    "                    # Converting to (x_min, y_min, x_max, y_max):\n",
    "                    pred_boxes_xyxy = torch.zeros_like(pred_boxes)\n",
    "                    pred_boxes_xyxy[:, 0] = pred_boxes[:, 0] - (0.5 * pred_boxes[:, 2])  # x_min\n",
    "                    pred_boxes_xyxy[:, 1] = pred_boxes[:, 1] - (0.5 * pred_boxes[:, 3])  # y_min\n",
    "                    pred_boxes_xyxy[:, 2] = pred_boxes[:, 0] + (0.5 * pred_boxes[:, 2])  # x_max\n",
    "                    pred_boxes_xyxy[:, 3] = pred_boxes[:, 1] + (0.5 * pred_boxes[:, 3])  # y_max\n",
    "\n",
    "                    # Creating the prediction dictionary:\n",
    "                    pred_dict = {\n",
    "                        'boxes': pred_boxes_xyxy,\n",
    "                        'labels': pred_labels,\n",
    "                        'scores': pred_scores,\n",
    "                    }\n",
    "                    pred.append(pred_dict)\n",
    "\n",
    "\n",
    "                # Updating the mean average precision:\n",
    "                anns_target = [{\"boxes\": ann[\"boxes\"], \"labels\": ann[\"class_labels\"]} for ann in anns]\n",
    "                map_epoch.update(pred, anns_target)\n",
    "\n",
    "\n",
    "            # Calculating the average loss for current epoch:\n",
    "            loss_epoch_avg = loss_epoch / len(training_data_loader)\n",
    "            training_losses.append(loss_epoch_avg.cpu().cpu().detach().numpy())\n",
    "\n",
    "            map_epoch_results = map_epoch.compute()\n",
    "            print(f\"Outputs From Last Batch: {output.loss_dict}\")\n",
    "            print(f\"Epoch {display_epoch} Average Loss: {loss_epoch_avg}\")\n",
    "            print(f\"Epoch {display_epoch} Mean Average Precision: {map_epoch_results['map']}, {map_epoch_results['map_50']}\")\n",
    "\n",
    "            training_maps.append(map_epoch_results['map'].cpu().detach().numpy())\n",
    "            training_maps_50.append(map_epoch_results['map_50'].cpu().detach().numpy())\n",
    "\n",
    "        \n",
    "            if validation_data_loader:\n",
    "                \n",
    "                loss_val = 0\n",
    "                map_val = MeanAveragePrecision()\n",
    "                self.model.eval() # Putting the model in evaluation mode\n",
    "\n",
    "                for idx, (imgs, anns) in enumerate(validation_data_loader):\n",
    "\n",
    "                    # Extracting height and width of each image in the batch as well as batch size:\n",
    "                    batch_size = imgs.shape[0]\n",
    "                    img_height = 512\n",
    "                    img_width = 512                 \n",
    "\n",
    "                    # Moving all input and target tensors to device:\n",
    "                    inputs = {\"pixel_values\": imgs.to(self.device)}\n",
    "                    anns = [{key: val.to(self.device) for key, val in ann.items()} for ann in anns]\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        \n",
    "                        # Forward pass:\n",
    "                        output = self.model(**inputs, labels=anns)\n",
    "\n",
    "                        # Loss from current batch:\n",
    "                        loss = output.loss\n",
    "                        loss_val += loss # Adding current loss to the total loss across the batch\n",
    "\n",
    "                        ### Check mAP:\n",
    "\n",
    "                        pred = []\n",
    "                        for ind in range(batch_size):\n",
    "                            pred_boxes = output.pred_boxes[ind]\n",
    "                            pred_scores = torch.max(torch.softmax(output.logits[ind], dim=-1), dim=-1).values\n",
    "                            pred_labels = torch.argmax(output.logits[ind], dim=-1)\n",
    "\n",
    "                            # Applying confidence threshold filtering:\n",
    "                            valid_indices = pred_scores > self.confidence_threshold\n",
    "                            pred_boxes = pred_boxes[valid_indices]\n",
    "                            pred_scores = pred_scores[valid_indices]\n",
    "                            pred_labels = pred_labels[valid_indices]\n",
    "\n",
    "                            # Denormalizing bounding boxes:\n",
    "                            pred_boxes[:, 0] *= img_width\n",
    "                            pred_boxes[:, 1] *= img_height\n",
    "                            pred_boxes[:, 2] *= img_width\n",
    "                            pred_boxes[:, 3] *= img_height\n",
    "\n",
    "                            # Converting to (x_min, y_min, x_max, y_max):\n",
    "                            pred_boxes_xyxy = torch.zeros_like(pred_boxes)\n",
    "                            pred_boxes_xyxy[:, 0] = pred_boxes[:, 0] - (0.5 * pred_boxes[:, 2])\n",
    "                            pred_boxes_xyxy[:, 1] = pred_boxes[:, 1] - (0.5 * pred_boxes[:, 3])\n",
    "                            pred_boxes_xyxy[:, 2] = pred_boxes[:, 0] + (0.5 * pred_boxes[:, 2])\n",
    "                            pred_boxes_xyxy[:, 3] = pred_boxes[:, 1] + (0.5 * pred_boxes[:, 3])\n",
    "\n",
    "                            pred_dict = {\n",
    "                                'boxes': pred_boxes_xyxy,\n",
    "                                'labels': pred_labels,\n",
    "                                'scores': pred_scores,\n",
    "                            }\n",
    "                            pred.append(pred_dict)\n",
    "\n",
    "\n",
    "                        # Updating the mean average precision:\n",
    "                        anns_target = [{\"boxes\": ann[\"boxes\"], \"labels\": ann[\"class_labels\"]} for ann in anns]\n",
    "                        map_val.update(pred, anns_target)\n",
    "\n",
    "\n",
    "            # Calculating average validation loss:\n",
    "            loss_val_avg = loss_val / len(validation_data_loader)\n",
    "            val_losses.append(loss_val_avg.cpu().detach().numpy())\n",
    "\n",
    "            map_val_results = map_val.compute()\n",
    "\n",
    "            val_maps.append(map_val_results['map'].cpu().detach().numpy())\n",
    "            val_maps_50.append(map_val_results['map_50'].cpu().detach().numpy())\n",
    "\n",
    "            print(f\"Validation Loss: {loss_val_avg}\")\n",
    "            print(f\"Validation Mean Average Precision: {map_val_results['map']}, {map_val_results['map_50']}\")\n",
    "\n",
    "\n",
    "            # Ensuring to save the model that achieves the lowest loss:\n",
    "            if loss_val_avg < self.best_loss:\n",
    "                self.best_loss = loss_val_avg\n",
    "                self.best_model_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        # Load best state after training for use:\n",
    "        if self.best_model_state_dict is not None:\n",
    "            self.model.load_state_dict(self.best_model_state_dict)\n",
    "\n",
    "        # Setting the training and validation metric lists to their respective class variables:\n",
    "        self.training_losses = training_losses\n",
    "        self.val_losses = val_losses\n",
    "        self.training_maps = training_maps\n",
    "        self.val_maps = val_maps\n",
    "        self.training_maps_50 = training_maps_50\n",
    "        self.val_maps_50 = val_maps_50\n",
    "\n",
    "\n",
    "    def test(self, test_data_loader):\n",
    "        \"\"\" This function applies the trained model to the given test data. \n",
    "            It prints and returns the test accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        loss_test = 0\n",
    "        map_test = MeanAveragePrecision()\n",
    "        self.model.eval() # Putting the model in evlauation mode\n",
    "\n",
    "        for idx, (imgs, anns) in enumerate(test_data_loader):\n",
    "            \n",
    "            # Moving all input and target tensors to device:\n",
    "            inputs = {\"pixel_values\": imgs.to(self.device)}\n",
    "            anns = [{key: val.to(self.device) for key, val in ann.items()} for ann in anns]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Forward pass:\n",
    "                output = self.model(**inputs, labels=anns)\n",
    "\n",
    "                # Loss from current batch:\n",
    "                loss = output.loss\n",
    "                loss_test += loss # Adding current loss to the total loss across the batch\n",
    "\n",
    "                ### Check MAP:\n",
    "\n",
    "                # Getting predictions:\n",
    "                pred = []\n",
    "                for ind in range(len(output.logits)):\n",
    "                    pred_dict = {}\n",
    "                    pred_dict['boxes'] = output.logits[ind]['boxes']\n",
    "                    pred_dict['labels'] = output.logits[ind]['labels']\n",
    "                    pred.append(pred_dict)\n",
    "\n",
    "                # Updating the mean average precision\n",
    "                map_test.update(pred, anns)\n",
    "\n",
    "        # Calculating average validation loss:\n",
    "        loss_test_avg = loss_test / len(test_data_loader)\n",
    "        map_test_results = map_test.compute()\n",
    "\n",
    "        print(f\"Test Loss: {loss_test_avg}\")\n",
    "        print(f\"Test Mean Average Precision: {map_test_results['map']}, {map_test_results['map_50']}\")\n",
    "\n",
    "\n",
    "    def plot_loss_curve(self, model_name):\n",
    "        \"\"\"\n",
    "        This function plots the loss curve from the most recent training period of this model manager.\n",
    "        \n",
    "        Inputs:\n",
    "        model_name (str) - Name of the model\n",
    "        \"\"\"\n",
    "\n",
    "        title = model_name + \" Loss Curve\"\n",
    "        filename = model_name + \"_loss_curve.png\"\n",
    "\n",
    "        # Moving tensors to CPU:\n",
    "        for i, values in enumerate(zip(self.training_losses, self.val_losses)):\n",
    "            self.training_losses[i] = values[0]\n",
    "            self.val_losses[i] = values[1]\n",
    "\n",
    "        # Plotting training and validation accuracy values:\n",
    "        plt.plot(self.training_losses, label='Training Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(title)\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(filename, dpi=600)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_map_curve(self, model_name):\n",
    "        \"\"\"\n",
    "        This function plots the map curve from the most recent training period of this model manager.\n",
    "        \n",
    "        Inputs:\n",
    "        model_name (str) - Name of the model\n",
    "        \"\"\"\n",
    "\n",
    "        title = model_name + \" mAP Curve\"\n",
    "        filename = model_name + \"_map_curve.png\"\n",
    "\n",
    "        # Moving tensors to CPU:\n",
    "        for i, values in enumerate(zip(self.training_maps, self.val_maps)):\n",
    "            self.training_maps[i] = values[0]\n",
    "            self.val_maps[i] = values[1]\n",
    "\n",
    "        # Plotting training and validation accuracy values:\n",
    "        plt.plot(self.training_maps, label='Training mAP')\n",
    "        plt.plot(self.val_maps, label='Validation mAP')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Mean Average Precision')\n",
    "        plt.title(title)\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(filename, dpi=600)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_map50_curve(self, model_name):\n",
    "        \"\"\"\n",
    "        This function plots the map50 curve from the most recent training period of this model manager.\n",
    "        \n",
    "        Inputs:\n",
    "        model_name (str) - Name of the model\n",
    "        \"\"\"\n",
    "\n",
    "        title = model_name + \" mAP50 Curve\"\n",
    "        filename = model_name + \"_map50_curve.png\"\n",
    "\n",
    "        # Moving tensors to CPU:\n",
    "        for i, values in enumerate(zip(self.training_maps_50, self.val_maps_50)):\n",
    "            self.training_maps_50[i] = values[0]\n",
    "            self.val_maps_50[i]=values[1]\n",
    "\n",
    "        # Plotting training and validation accuracy values:\n",
    "        plt.plot(self.training_maps_50, label='Training mAP50')\n",
    "        plt.plot(self.val_maps_50, label='Validation mAP50')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Mean Average Precision 50')\n",
    "        plt.title(title)\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(filename, dpi=600)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def save(self, filepath):\n",
    "        if self.best_model_state_dict is None:\n",
    "            torch.save(self.model.state_dict(), filepath)\n",
    "        else:\n",
    "            torch.save(self.best_model_state_dict, filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        self.model.load_state_dict(torch.load(filepath, weights_only=True))\n",
    "\n",
    "    def prompt_llm(self, model_preds, prompt):\n",
    "        \"\"\"prompts a llm for food nutrition facts, health benefits, and recipes\n",
    "\n",
    "        Args:\n",
    "            model_preds (string): model prediciton. the food label\n",
    "\n",
    "        Returns:\n",
    "            str: llm's response\n",
    "        \"\"\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt3.5-turbo\")\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\"gpt3.5-turbo\")\n",
    "        \n",
    "        prompt = f\"{prompt} {model_preds}\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = llm.generate(**inputs, max_length=500)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Detected Food Item: {detected_items}\")\n",
    "        print(f\"Requested Data:\\n{response}\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36942380-5c19-4213-8d50-d40dec1a1aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08  02:58:16 INFO Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2024-12-08  02:58:16 INFO [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-08  02:58:16 INFO Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading DETR Resnet-50 Model from HuggingFace:\n",
    "\n",
    "# detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", num_labels=len(detr_processor.classes)+1, num_queries=25, ignore_mismatched_sizes=True)\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "# detr_model.config.no_object_weight = 0.1\n",
    "optimizer = optim.AdamW(detr_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Freezing the ResNet backbone:\n",
    "for param in detr_model.model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "detr_wrapper = DETRModelManager(model=detr_model, optimizer=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9084c28-fc88-4b7f-8a53-9af341b046a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs From Last Batch: {'loss_ce': tensor(1.3661, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox': tensor(0.3250, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7074, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(3.3333, device='cuda:0')}\n",
      "Epoch 1 Average Loss: 5.740235805511475\n",
      "Epoch 1 Mean Average Precision: 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [01:08<55:32, 68.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.358926296234131\n",
      "Validation Mean Average Precision: 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [01:29<1:13:26, 89.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training the model:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdetr_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 67\u001b[0m, in \u001b[0;36mDETRModelManager.train\u001b[1;34m(self, training_data_loader, validation_data_loader, epochs)\u001b[0m\n\u001b[0;32m     64\u001b[0m anns \u001b[38;5;241m=\u001b[39m [{key: val\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m ann\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m anns]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Forward pass:\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Loss from current batch:\u001b[39;00m\n\u001b[0;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:1441\u001b[0m, in \u001b[0;36mDetrForObjectDetection.forward\u001b[1;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1438\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1440\u001b[0m \u001b[38;5;66;03m# First, sent images through DETR base model to obtain encoder + decoder outputs\u001b[39;00m\n\u001b[1;32m-> 1441\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1453\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;66;03m# class logits + predicted bounding boxes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:1284\u001b[0m, in \u001b[0;36mDetrModel.forward\u001b[1;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# Fourth, sent flattened_features + flattened_mask + position embeddings through encoder\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;66;03m# flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;66;03m# flattened_mask is a Tensor of shape (batch_size, heigth*width)\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflattened_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflattened_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobject_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:966\u001b[0m, in \u001b[0;36mDetrEncoder.forward\u001b[1;34m(self, inputs_embeds, attention_mask, object_queries, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    963\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;66;03m# we add object_queries as extra input to the encoder_layer\u001b[39;00m\n\u001b[1;32m--> 966\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobject_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\miniconda3\\envs\\conv_test\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:673\u001b[0m, in \u001b[0;36mDetrEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, object_queries, output_attentions)\u001b[0m\n\u001b[0;32m    670\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m--> 673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(hidden_states)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    674\u001b[0m         clamp_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(hidden_states\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmax \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    675\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(hidden_states, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mclamp_value, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mclamp_value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model:\n",
    "detr_wrapper.train(training_data_loader=train_set, validation_data_loader=val_set, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24319b6-feb1-4744-85b7-4be33979e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating, saving, and displaying loss curve from training:\n",
    "detr_wrapper.plot_loss_curve(\"DERT\")\n",
    "\n",
    "# Creating, saving, and displaying mAP curve from training:\n",
    "detr_wrapper.plot_map_curve(\"DERT\")\n",
    "\n",
    "# Creating, saving, and displaying mAP50 curve from training:\n",
    "detr_wrapper.plot_map50_curve(\"DERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268eba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5be5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model:\n",
    "detr_wrapper.test(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d8705-c919-4e09-9b92-8e73933fce32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
